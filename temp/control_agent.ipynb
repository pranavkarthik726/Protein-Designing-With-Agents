{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a17cbffb",
   "metadata": {},
   "source": [
    "ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baf367d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mControl Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mPerform a diagnostic check of the system and report the status. Aggregate results from any subordinate tasks if applicable.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mControl Agent\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "```\n",
      "## System Diagnostic Report\n",
      "\n",
      "**Date:** 2024-07-18 (Example - please update with actual date)\n",
      "**Time:** 14:35 UTC (Example - please update with actual time)\n",
      "\n",
      "**Overall System Status:** [Insert Overall Status - e.g., Operational, Degraded, Critical]\n",
      "\n",
      "**Component Status:**\n",
      "\n",
      "*   **CPU:**\n",
      "    *   Status: [e.g., Normal, Overloaded, Faulty]\n",
      "    *   Utilization: [e.g., 25%, 95%, N/A]\n",
      "    *   Temperature: [e.g., 45°C, 90°C, N/A]\n",
      "*   **Memory:**\n",
      "    *   Status: [e.g., Normal, High Usage, Errors Detected]\n",
      "    *   Utilization: [e.g., 60%, 99%, N/A]\n",
      "    *   Available Memory: [e.g., 4GB, 100MB, N/A]\n",
      "*   **Storage:**\n",
      "    *   Status: [e.g., Normal, Low Disk Space, Read-Only]\n",
      "    *   Disk Utilization: [e.g., 70%, 98%, N/A]\n",
      "    *   Available Disk Space: [e.g., 100GB, 5GB, N/A]\n",
      "    *   I/O Performance: [e.g., Normal, Degraded, N/A]\n",
      "*   **Network:**\n",
      "    *   Status: [e.g., Connected, Disconnected, Intermittent]\n",
      "    *   Latency: [e.g., 10ms, 500ms, N/A]\n",
      "    *   Bandwidth: [e.g., 1Gbps, 10Mbps, N/A]\n",
      "    *   Packet Loss: [e.g., 0%, 5%, N/A]\n",
      "*   **Operating System:**\n",
      "    *   Status: [e.g., Running, Responding, Unresponsive]\n",
      "    *   Version: [e.g., Linux 5.4.0-100-generic, Windows 10, N/A]\n",
      "    *   Uptime: [e.g., 2 days, 1 hour, N/A]\n",
      "*   **Critical Processes:**\n",
      "    *   [Process Name 1]: Status - [e.g., Running, Stopped, Faulty], CPU Usage - [e.g., 10%, N/A], Memory Usage - [e.g., 1GB, N/A]\n",
      "    *   [Process Name 2]: Status - [e.g., Running, Stopped, Faulty], CPU Usage - [e.g., 5%, N/A], Memory Usage - [e.g., 500MB, N/A]\n",
      "    *   [Add more processes as needed]\n",
      "\n",
      "**Diagnostic Findings:**\n",
      "\n",
      "*   [Detail any specific errors, warnings, or anomalies detected during the diagnostic check.  Include timestamps and relevant error codes where applicable.  For example: \"Error: Disk I/O latency exceeded threshold at 14:30 UTC (Error Code: DISK_IO_001)\"]\n",
      "*   [Detail any specific errors, warnings, or anomalies detected during the diagnostic check.  Include timestamps and relevant error codes where applicable. For example: \"Warning: CPU temperature approaching critical level at 14:32 UTC.\"]\n",
      "*   [Add more findings as needed]\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "*   [Provide recommendations based on the diagnostic findings. For example: \"Investigate high disk I/O latency.\"]\n",
      "*   [Provide recommendations based on the diagnostic findings. For example: \"Monitor CPU temperature and consider cooling improvements.\"]\n",
      "*   [Add more recommendations as needed]\n",
      "\n",
      "**Notes:**\n",
      "\n",
      "*   [Include any additional notes or context relevant to the system status and diagnostic findings. For example: \"System was recently updated to the latest software version.\"]\n",
      "*   [Include any additional notes or context relevant to the system status and diagnostic findings. For example: \"Network connectivity issues reported by users.\"]\n",
      "\n",
      "**End of Report**\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "CrewAI Control Agent Output:\n",
      "```\n",
      "## System Diagnostic Report\n",
      "\n",
      "**Date:** 2024-07-18 (Example - please update with actual date)\n",
      "**Time:** 14:35 UTC (Example - please update with actual time)\n",
      "\n",
      "**Overall System Status:** [Insert Overall Status - e.g., Operational, Degraded, Critical]\n",
      "\n",
      "**Component Status:**\n",
      "\n",
      "*   **CPU:**\n",
      "    *   Status: [e.g., Normal, Overloaded, Faulty]\n",
      "    *   Utilization: [e.g., 25%, 95%, N/A]\n",
      "    *   Temperature: [e.g., 45°C, 90°C, N/A]\n",
      "*   **Memory:**\n",
      "    *   Status: [e.g., Normal, High Usage, Errors Detected]\n",
      "    *   Utilization: [e.g., 60%, 99%, N/A]\n",
      "    *   Available Memory: [e.g., 4GB, 100MB, N/A]\n",
      "*   **Storage:**\n",
      "    *   Status: [e.g., Normal, Low Disk Space, Read-Only]\n",
      "    *   Disk Utilization: [e.g., 70%, 98%, N/A]\n",
      "    *   Available Disk Space: [e.g., 100GB, 5GB, N/A]\n",
      "    *   I/O Performance: [e.g., Normal, Degraded, N/A]\n",
      "*   **Network:**\n",
      "    *   Status: [e.g., Connected, Disconnected, Intermittent]\n",
      "    *   Latency: [e.g., 10ms, 500ms, N/A]\n",
      "    *   Bandwidth: [e.g., 1Gbps, 10Mbps, N/A]\n",
      "    *   Packet Loss: [e.g., 0%, 5%, N/A]\n",
      "*   **Operating System:**\n",
      "    *   Status: [e.g., Running, Responding, Unresponsive]\n",
      "    *   Version: [e.g., Linux 5.4.0-100-generic, Windows 10, N/A]\n",
      "    *   Uptime: [e.g., 2 days, 1 hour, N/A]\n",
      "*   **Critical Processes:**\n",
      "    *   [Process Name 1]: Status - [e.g., Running, Stopped, Faulty], CPU Usage - [e.g., 10%, N/A], Memory Usage - [e.g., 1GB, N/A]\n",
      "    *   [Process Name 2]: Status - [e.g., Running, Stopped, Faulty], CPU Usage - [e.g., 5%, N/A], Memory Usage - [e.g., 500MB, N/A]\n",
      "    *   [Add more processes as needed]\n",
      "\n",
      "**Diagnostic Findings:**\n",
      "\n",
      "*   [Detail any specific errors, warnings, or anomalies detected during the diagnostic check.  Include timestamps and relevant error codes where applicable.  For example: \"Error: Disk I/O latency exceeded threshold at 14:30 UTC (Error Code: DISK_IO_001)\"]\n",
      "*   [Detail any specific errors, warnings, or anomalies detected during the diagnostic check.  Include timestamps and relevant error codes where applicable. For example: \"Warning: CPU temperature approaching critical level at 14:32 UTC.\"]\n",
      "*   [Add more findings as needed]\n",
      "\n",
      "**Recommendations:**\n",
      "\n",
      "*   [Provide recommendations based on the diagnostic findings. For example: \"Investigate high disk I/O latency.\"]\n",
      "*   [Provide recommendations based on the diagnostic findings. For example: \"Monitor CPU temperature and consider cooling improvements.\"]\n",
      "*   [Add more recommendations as needed]\n",
      "\n",
      "**Notes:**\n",
      "\n",
      "*   [Include any additional notes or context relevant to the system status and diagnostic findings. For example: \"System was recently updated to the latest software version.\"]\n",
      "*   [Include any additional notes or context relevant to the system status and diagnostic findings. For example: \"Network connectivity issues reported by users.\"]\n",
      "\n",
      "**End of Report**\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai import LLM\n",
    "\n",
    "# Set your Gemini API key securely (avoid hardcoding in production)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyABZOPrSLEGtDNroZPk_94PfHV4f-5-luY\"  # Replace with your secure API key\n",
    "\n",
    "# Initialize the Gemini 2.0 Flash model using CrewAI’s LLM wrapper\n",
    "llm = LLM(\n",
    "    model=\"gemini/gemini-2.0-flash\",\n",
    "    verbose=True,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "# Define a control agent that will act as the crew manager\n",
    "control_agent = Agent(\n",
    "    role=\"Control Agent\",\n",
    "    goal=\"Monitor and manage crew operations and aggregate results\",\n",
    "    backstory=\"You are a control agent overseeing a multi-agent crew. Your job is to initiate tasks and compile a summary of their outputs.\",\n",
    "    llm=llm,\n",
    "    allow_delegation=True,\n",
    "    verbose=True,\n",
    "    memory=False  # Set to True if you require memory persistence\n",
    ")\n",
    "\n",
    "# Define a task for the control agent\n",
    "control_task = Task(\n",
    "    description=\"Perform a diagnostic check of the system and report the status. Aggregate results from any subordinate tasks if applicable.\",\n",
    "    expected_output=\"A concise summary report detailing the system status and any diagnostic findings.\",\n",
    "    agent=control_agent\n",
    ")\n",
    "\n",
    "# Create a crew with the control agent and assign the task\n",
    "crew = Crew(\n",
    "    agents=[control_agent],\n",
    "    tasks=[control_task],\n",
    "    process=Process.sequential,  # Tasks run one after the other\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Kickoff the crew operation with sample input\n",
    "result = crew.kickoff(inputs={\"task\": \"Perform a system diagnostic and report status\"})\n",
    "print(\"CrewAI Control Agent Output:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b92cbf7",
   "metadata": {},
   "source": [
    "Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adfef305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mResearch the latest developments in AI\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mDelegate work to coworker\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"task\\\": {\\\"description\\\": \\\"Research the latest developments in AI and provide a list of key findings.\\\", \\\"type\\\": \\\"str\\\"}, \\\"context\\\": {\\\"description\\\": \\\"The goal is to compile a list of key findings related to the latest advancements in Artificial Intelligence.  This should cover a broad range of AI subfields, including but not limited to:  natural language processing (NLP), computer vision, machine learning (ML), deep learning (DL), reinforcement learning (RL), generative AI, and explainable AI (XAI). The key findings should be concise and impactful, focusing on significant breakthroughs, new techniques, or important applications. Please prioritize recent developments within the last 12 months.  The final output should be a numbered list of key findings.\\\", \\\"type\\\": \\\"str\\\"}, \\\"coworker\\\": {\\\"description\\\": \\\"Researcher\\\", \\\"type\\\": \\\"str\\\"}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "\n",
      "I encountered an error while trying to use the tool. This was the error: unhashable type: 'dict'.\n",
      " Tool Delegate work to coworker accepts these inputs: Tool Name: Delegate work to coworker\n",
      "Tool Arguments: {'task': {'description': 'The task to delegate', 'type': 'str'}, 'context': {'description': 'The context for the task', 'type': 'str'}, 'coworker': {'description': 'The role/name of the coworker to delegate to', 'type': 'str'}}\n",
      "Tool Description: Delegate a specific task to one of the following coworkers: Researcher\n",
      "The input to this tool should be the coworker, the task you want them to do, and ALL necessary context to execute the task, they know nothing about the task, so share absolutely everything you know, don't reference things but instead explain them..\n",
      "Moving on then. I MUST either use a tool (use one at time) OR give my best final answer not both at the same time. When responding, I must use the following format:\n",
      "\n",
      "```\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Delegate work to coworker, Ask question to coworker]\n",
      "Action Input: the input to the action, dictionary enclosed in curly braces\n",
      "Observation: the result of the action\n",
      "```\n",
      "This Thought/Action/Action Input/Result can repeat N times. Once I know the final answer, I must return the following format:\n",
      "\n",
      "```\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: Your final answer must be the great and the most complete as possible, it must be outcome described\n",
      "\n",
      "```\u001b[00m\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mResearch the latest developments in AI and provide a list of key findings.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "1. **Significant advancements in Large Language Models (LLMs):**  The past year has seen considerable progress in LLMs, with models like GPT-4 demonstrating improved reasoning, fewer hallucinations, and enhanced multilingual capabilities.  Research focuses on improving efficiency (reducing computational costs), enhancing context windows for longer-term memory, and mitigating biases and safety concerns.  Methods like reinforcement learning from human feedback (RLHF) and instruction tuning have played crucial roles.\n",
      "\n",
      "\n",
      "2. **Emergence of Multimodal AI:**  We've witnessed a surge in multimodal AI systems that seamlessly integrate different modalities like text, images, and audio. These models can understand and generate content across various formats, leading to more sophisticated and natural interactions.  For example, models can now generate images from text descriptions with unprecedented detail and accuracy, or translate between different languages while considering visual context.\n",
      "\n",
      "\n",
      "3. **Progress in Explainable AI (XAI):**  The demand for transparency and interpretability in AI systems has driven significant research in XAI. New methods are being developed to provide insights into the decision-making processes of complex models, particularly deep learning models. This includes techniques to visualize internal representations, generate explanations in natural language, and quantify uncertainty.  This is crucial for building trust and enabling responsible AI deployment.\n",
      "\n",
      "\n",
      "4. **Improvements in Generative AI for various applications:** Generative models continue to excel beyond image and text generation. We are seeing significant progress in areas like:\n",
      "    * **Generative design:** AI is increasingly used to generate novel designs in engineering, architecture, and product design, optimizing for performance and aesthetics.\n",
      "    * **Drug discovery and materials science:** Generative models are accelerating the discovery of new drugs and materials by predicting molecular properties and designing novel compounds.\n",
      "    * **Synthetic data generation:** This is becoming critical for training AI models in data-scarce domains, ensuring privacy, and augmenting existing datasets.\n",
      "\n",
      "\n",
      "5. **Reinforcement Learning (RL) breakthroughs in robotics:** RL is enabling significant progress in robotics, allowing robots to learn complex tasks through trial and error and adapt to dynamic environments.  This includes improvements in robot manipulation, locomotion, and navigation, pushing the boundaries of autonomous systems.  Simulation-to-reality transfer is a key area of focus to accelerate learning and reduce the reliance on real-world data.\n",
      "\n",
      "\n",
      "6. **Focus on AI Safety and Ethics:**  The growing capabilities of AI systems have highlighted the importance of ethical considerations and safety protocols.  Research is focused on developing techniques to mitigate biases, ensure fairness, and prevent malicious use.  This includes developing robust methods for detecting and preventing adversarial attacks and creating frameworks for responsible AI development and deployment.\n",
      "\n",
      "\n",
      "7. **Edge AI advancements:**  The increasing power and efficiency of edge devices (smartphones, IoT devices) are enabling the deployment of more sophisticated AI models directly on these devices. This reduces latency, improves privacy, and enables real-time applications in contexts with limited connectivity.\n",
      "\n",
      "\n",
      "8. **Neuro-Symbolic AI:** This emerging field aims to combine the strengths of neural networks (learning from data) and symbolic AI (reasoning and knowledge representation). This approach promises to create more robust, explainable, and knowledge-rich AI systems, capable of handling complex reasoning tasks.\n",
      "\n",
      "\n",
      "9. **Advances in Computer Vision for 3D understanding:**  Computer vision is advancing beyond 2D image processing, with a greater focus on 3D scene understanding.  This includes improvements in depth estimation, 3D object detection, and scene reconstruction, facilitating applications in autonomous driving, robotics, and augmented reality.\n",
      "\n",
      "\n",
      "10. **Increased focus on data efficiency:**  Research is increasingly focusing on developing AI models that require less data for training. This addresses the challenges of data scarcity, cost, and privacy, making AI more accessible and applicable to a wider range of problems.  Techniques like transfer learning, few-shot learning, and meta-learning are playing a key role.\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought:I will try again to delegate the task, correcting the input format.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mDelegate work to coworker\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"task\\\": \\\"Research the latest developments in AI and provide a list of key findings.\\\", \\\"context\\\": \\\"The goal is to compile a list of key findings related to the latest advancements in Artificial Intelligence. This should cover a broad range of AI subfields, including but not limited to: natural language processing (NLP), computer vision, machine learning (ML), deep learning (DL), reinforcement learning (RL), generative AI, and explainable AI (XAI). The key findings should be concise and impactful, focusing on significant breakthroughs, new techniques, or important applications. Please prioritize recent developments within the last 12 months. The final output should be a numbered list of key findings.\\\", \\\"coworker\\\": \\\"Researcher\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "1. **Significant advancements in Large Language Models (LLMs):**  The past year has seen considerable progress in LLMs, with models like GPT-4 demonstrating improved reasoning, fewer hallucinations, and enhanced multilingual capabilities.  Research focuses on improving efficiency (reducing computational costs), enhancing context windows for longer-term memory, and mitigating biases and safety concerns.  Methods like reinforcement learning from human feedback (RLHF) and instruction tuning have played crucial roles.\n",
      "\n",
      "\n",
      "2. **Emergence of Multimodal AI:**  We've witnessed a surge in multimodal AI systems that seamlessly integrate different modalities like text, images, and audio. These models can understand and generate content across various formats, leading to more sophisticated and natural interactions.  For example, models can now generate images from text descriptions with unprecedented detail and accuracy, or translate between different languages while considering visual context.\n",
      "\n",
      "\n",
      "3. **Progress in Explainable AI (XAI):**  The demand for transparency and interpretability in AI systems has driven significant research in XAI. New methods are being developed to provide insights into the decision-making processes of complex models, particularly deep learning models. This includes techniques to visualize internal representations, generate explanations in natural language, and quantify uncertainty.  This is crucial for building trust and enabling responsible AI deployment.\n",
      "\n",
      "\n",
      "4. **Improvements in Generative AI for various applications:** Generative models continue to excel beyond image and text generation. We are seeing significant progress in areas like:\n",
      "    * **Generative design:** AI is increasingly used to generate novel designs in engineering, architecture, and product design, optimizing for performance and aesthetics.\n",
      "    * **Drug discovery and materials science:** Generative models are accelerating the discovery of new drugs and materials by predicting molecular properties and designing novel compounds.\n",
      "    * **Synthetic data generation:** This is becoming critical for training AI models in data-scarce domains, ensuring privacy, and augmenting existing datasets.\n",
      "\n",
      "\n",
      "5. **Reinforcement Learning (RL) breakthroughs in robotics:** RL is enabling significant progress in robotics, allowing robots to learn complex tasks through trial and error and adapt to dynamic environments.  This includes improvements in robot manipulation, locomotion, and navigation, pushing the boundaries of autonomous systems.  Simulation-to-reality transfer is a key area of focus to accelerate learning and reduce the reliance on real-world data.\n",
      "\n",
      "\n",
      "6. **Focus on AI Safety and Ethics:**  The growing capabilities of AI systems have highlighted the importance of ethical considerations and safety protocols.  Research is focused on developing techniques to mitigate biases, ensure fairness, and prevent malicious use.  This includes developing robust methods for detecting and preventing adversarial attacks and creating frameworks for responsible AI development and deployment.\n",
      "\n",
      "\n",
      "7. **Edge AI advancements:**  The increasing power and efficiency of edge devices (smartphones, IoT devices) are enabling the deployment of more sophisticated AI models directly on these devices. This reduces latency, improves privacy, and enables real-time applications in contexts with limited connectivity.\n",
      "\n",
      "\n",
      "8. **Neuro-Symbolic AI:** This emerging field aims to combine the strengths of neural networks (learning from data) and symbolic AI (reasoning and knowledge representation). This approach promises to create more robust, explainable, and knowledge-rich AI systems, capable of handling complex reasoning tasks.\n",
      "\n",
      "\n",
      "9. **Advances in Computer Vision for 3D understanding:**  Computer vision is advancing beyond 2D image processing, with a greater focus on 3D scene understanding.  This includes improvements in depth estimation, 3D object detection, and scene reconstruction, facilitating applications in autonomous driving, robotics, and augmented reality.\n",
      "\n",
      "\n",
      "10. **Increased focus on data efficiency:**  Research is increasingly focusing on developing AI models that require less data for training. This addresses the challenges of data scarcity, cost, and privacy, making AI more accessible and applicable to a wider range of problems.  Techniques like transfer learning, few-shot learning, and meta-learning are playing a key role.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "1. **Significant advancements in Large Language Models (LLMs):**  The past year has seen considerable progress in LLMs, with models like GPT-4 demonstrating improved reasoning, fewer hallucinations, and enhanced multilingual capabilities.  Research focuses on improving efficiency (reducing computational costs), enhancing context windows for longer-term memory, and mitigating biases and safety concerns.  Methods like reinforcement learning from human feedback (RLHF) and instruction tuning have played crucial roles.\n",
      "\n",
      "\n",
      "2. **Emergence of Multimodal AI:**  We've witnessed a surge in multimodal AI systems that seamlessly integrate different modalities like text, images, and audio. These models can understand and generate content across various formats, leading to more sophisticated and natural interactions.  For example, models can now generate images from text descriptions with unprecedented detail and accuracy, or translate between different languages while considering visual context.\n",
      "\n",
      "\n",
      "3. **Progress in Explainable AI (XAI):**  The demand for transparency and interpretability in AI systems has driven significant research in XAI. New methods are being developed to provide insights into the decision-making processes of complex models, particularly deep learning models. This includes techniques to visualize internal representations, generate explanations in natural language, and quantify uncertainty.  This is crucial for building trust and enabling responsible AI deployment.\n",
      "\n",
      "\n",
      "4. **Improvements in Generative AI for various applications:** Generative models continue to excel beyond image and text generation. We are seeing significant progress in areas like:\n",
      "    * **Generative design:** AI is increasingly used to generate novel designs in engineering, architecture, and product design, optimizing for performance and aesthetics.\n",
      "    * **Drug discovery and materials science:** Generative models are accelerating the discovery of new drugs and materials by predicting molecular properties and designing novel compounds.\n",
      "    * **Synthetic data generation:** This is becoming critical for training AI models in data-scarce domains, ensuring privacy, and augmenting existing datasets.\n",
      "\n",
      "\n",
      "5. **Reinforcement Learning (RL) breakthroughs in robotics:** RL is enabling significant progress in robotics, allowing robots to learn complex tasks through trial and error and adapt to dynamic environments.  This includes improvements in robot manipulation, locomotion, and navigation, pushing the boundaries of autonomous systems.  Simulation-to-reality transfer is a key area of focus to accelerate learning and reduce the reliance on real-world data.\n",
      "\n",
      "\n",
      "6. **Focus on AI Safety and Ethics:**  The growing capabilities of AI systems have highlighted the importance of ethical considerations and safety protocols.  Research is focused on developing techniques to mitigate biases, ensure fairness, and prevent malicious use.  This includes developing robust methods for detecting and preventing adversarial attacks and creating frameworks for responsible AI development and deployment.\n",
      "\n",
      "\n",
      "7. **Edge AI advancements:**  The increasing power and efficiency of edge devices (smartphones, IoT devices) are enabling the deployment of more sophisticated AI models directly on these devices. This reduces latency, improves privacy, and enables real-time applications in contexts with limited connectivity.\n",
      "\n",
      "\n",
      "8. **Neuro-Symbolic AI:** This emerging field aims to combine the strengths of neural networks (learning from data) and symbolic AI (reasoning and knowledge representation). This approach promises to create more robust, explainable, and knowledge-rich AI systems, capable of handling complex reasoning tasks.\n",
      "\n",
      "\n",
      "9. **Advances in Computer Vision for 3D understanding:**  Computer vision is advancing beyond 2D image processing, with a greater focus on 3D scene understanding.  This includes improvements in depth estimation, 3D object detection, and scene reconstruction, facilitating applications in autonomous driving, robotics, and augmented reality.\n",
      "\n",
      "\n",
      "10. **Increased focus on data efficiency:**  Research is increasingly focusing on developing AI models that require less data for training. This addresses the challenges of data scarcity, cost, and privacy, making AI more accessible and applicable to a wider range of problems.  Techniques like transfer learning, few-shot learning, and meta-learning are playing a key role.\n",
      "\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mWrite a summary based on the research findings\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mDelegate work to coworker\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"task\\\": {\\\"description\\\": \\\"Write a concise summary of the following research findings, focusing on the key advancements and highlighting the most significant progress in each area.  Avoid excessive detail and maintain a clear, concise style.\\\", \\\"type\\\": \\\"str\\\"}, \\\"context\\\": {\\\"description\\\": \\\"The following are the research findings:\\\\n\\\\n1. **Significant advancements in Large Language Models (LLMs):**  The past year has seen considerable progress in LLMs, with models like GPT-4 demonstrating improved reasoning, fewer hallucinations, and enhanced multilingual capabilities.  Research focuses on improving efficiency (reducing computational costs), enhancing context windows for longer-term memory, and mitigating biases and safety concerns.  Methods like reinforcement learning from human feedback (RLHF) and instruction tuning have played crucial roles.\\\\n\\\\n2. **Emergence of Multimodal AI:**  We've witnessed a surge in multimodal AI systems that seamlessly integrate different modalities like text, images, and audio. These models can understand and generate content across various formats, leading to more sophisticated and natural interactions.  For example, models can now generate images from text descriptions with unprecedented detail and accuracy, or translate between different languages while considering visual context.\\\\n\\\\n3. **Progress in Explainable AI (XAI):**  The demand for transparency and interpretability in AI systems has driven significant research in XAI. New methods are being developed to provide insights into the decision-making processes of complex models, particularly deep learning models. This includes techniques to visualize internal representations, generate explanations in natural language, and quantify uncertainty.  This is crucial for building trust and enabling responsible AI deployment.\\\\n\\\\n4. **Improvements in Generative AI for various applications:** Generative models continue to excel beyond image and text generation. We are seeing significant progress in areas like:\\\\n    * **Generative design:** AI is increasingly used to generate novel designs in engineering, architecture, and product design, optimizing for performance and aesthetics.\\\\n    * **Drug discovery and materials science:** Generative models are accelerating the discovery of new drugs and materials by predicting molecular properties and designing novel compounds.\\\\n    * **Synthetic data generation:** This is becoming critical for training AI models in data-scarce domains, ensuring privacy, and augmenting existing datasets.\\\\n\\\\n5. **Reinforcement Learning (RL) breakthroughs in robotics:** RL is enabling significant progress in robotics, allowing robots to learn complex tasks through trial and error and adapt to dynamic environments.  This includes improvements in robot manipulation, locomotion, and navigation, pushing the boundaries of autonomous systems.  Simulation-to-reality transfer is a key area of focus to accelerate learning and reduce the reliance on real-world data.\\\\n\\\\n6. **Focus on AI Safety and Ethics:**  The growing capabilities of AI systems have highlighted the importance of ethical considerations and safety protocols.  Research is focused on developing techniques to mitigate biases, ensure fairness, and prevent malicious use.  This includes developing robust methods for detecting and preventing adversarial attacks and creating frameworks for responsible AI development and deployment.\\\\n\\\\n7. **Edge AI advancements:**  The increasing power and efficiency of edge devices (smartphones, IoT devices) are enabling the deployment of more sophisticated AI models directly on these devices. This reduces latency, improves privacy, and enables real-time applications in contexts with limited connectivity.\\\\n\\\\n8. **Neuro-Symbolic AI:** This emerging field aims to combine the strengths of neural networks (learning from data) and symbolic AI (reasoning and knowledge representation). This approach promises to create more robust, explainable, and knowledge-rich AI systems, capable of handling complex reasoning tasks.\\\\n\\\\n9. **Advances in Computer Vision for 3D understanding:**  Computer vision is advancing beyond 2D image processing, with a greater focus on 3D scene understanding.  This includes improvements in depth estimation, 3D object detection, and scene reconstruction, facilitating applications in autonomous driving, robotics, and augmented reality.\\\\n\\\\n10. **Increased focus on data efficiency:**  Research is increasingly focusing on developing AI models that require less data for training. This addresses the challenges of data scarcity, cost, and privacy, making AI more accessible and applicable to a wider range of problems.  Techniques like transfer learning, few-shot learning, and meta-learning are playing a key role.\\\", \\\"type\\\": \\\"str\\\"}, \\\"coworker\\\": {\\\"description\\\": \\\"Writer\\\", \\\"type\\\": \\\"str\\\"}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "\n",
      "I encountered an error while trying to use the tool. This was the error: unhashable type: 'dict'.\n",
      " Tool Delegate work to coworker accepts these inputs: Tool Name: Delegate work to coworker\n",
      "Tool Arguments: {'task': {'description': 'The task to delegate', 'type': 'str'}, 'context': {'description': 'The context for the task', 'type': 'str'}, 'coworker': {'description': 'The role/name of the coworker to delegate to', 'type': 'str'}}\n",
      "Tool Description: Delegate a specific task to one of the following coworkers: Writer\n",
      "The input to this tool should be the coworker, the task you want them to do, and ALL necessary context to execute the task, they know nothing about the task, so share absolutely everything you know, don't reference things but instead explain them..\n",
      "Moving on then. I MUST either use a tool (use one at time) OR give my best final answer not both at the same time. When responding, I must use the following format:\n",
      "\n",
      "```\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Delegate work to coworker, Ask question to coworker]\n",
      "Action Input: the input to the action, dictionary enclosed in curly braces\n",
      "Observation: the result of the action\n",
      "```\n",
      "This Thought/Action/Action Input/Result can repeat N times. Once I know the final answer, I must return the following format:\n",
      "\n",
      "```\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: Your final answer must be the great and the most complete as possible, it must be outcome described\n",
      "\n",
      "```\u001b[00m\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mWriter\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mWrite a concise summary of the following research findings, focusing on the key advancements and highlighting the most significant progress in each area.  Avoid excessive detail and maintain a clear, concise style.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mWriter\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "The past year has witnessed significant advancements across various AI domains.  Large Language Models (LLMs) show improved reasoning, multilingual capabilities, and reduced biases, driven by techniques like RLHF and instruction tuning.  Multimodal AI integrates text, images, and audio for more natural interactions, while Explainable AI (XAI) focuses on enhancing transparency and interpretability. Generative AI excels in diverse applications, including generative design, drug discovery, and synthetic data generation.  Reinforcement Learning (RL) breakthroughs are propelling progress in robotics, particularly in manipulation and locomotion.  A strong emphasis on AI safety and ethics guides research to mitigate biases and prevent malicious use. Edge AI empowers the deployment of sophisticated models on resource-constrained devices.  Neuro-symbolic AI aims to combine the strengths of neural networks and symbolic AI for more robust systems.  Computer vision is progressing toward 3D scene understanding, and a growing focus on data efficiency is making AI more accessible through techniques like transfer learning.\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mDelegate work to coworker\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"task\\\": \\\"Write a concise summary of the following research findings, focusing on the key advancements and highlighting the most significant progress in each area.  Avoid excessive detail and maintain a clear, concise style.\\\", \\\"context\\\": \\\"1. **Significant advancements in Large Language Models (LLMs):**  The past year has seen considerable progress in LLMs, with models like GPT-4 demonstrating improved reasoning, fewer hallucinations, and enhanced multilingual capabilities.  Research focuses on improving efficiency (reducing computational costs), enhancing context windows for longer-term memory, and mitigating biases and safety concerns.  Methods like reinforcement learning from human feedback (RLHF) and instruction tuning have played crucial roles.\\\\n\\\\n2. **Emergence of Multimodal AI:**  We've witnessed a surge in multimodal AI systems that seamlessly integrate different modalities like text, images, and audio. These models can understand and generate content across various formats, leading to more sophisticated and natural interactions.  For example, models can now generate images from text descriptions with unprecedented detail and accuracy, or translate between different languages while considering visual context.\\\\n\\\\n3. **Progress in Explainable AI (XAI):**  The demand for transparency and interpretability in AI systems has driven significant research in XAI. New methods are being developed to provide insights into the decision-making processes of complex models, particularly deep learning models. This includes techniques to visualize internal representations, generate explanations in natural language, and quantify uncertainty.  This is crucial for building trust and enabling responsible AI deployment.\\\\n\\\\n4. **Improvements in Generative AI for various applications:** Generative models continue to excel beyond image and text generation. We are seeing significant progress in areas like:\\\\n    * **Generative design:** AI is increasingly used to generate novel designs in engineering, architecture, and product design, optimizing for performance and aesthetics.\\\\n    * **Drug discovery and materials science:** Generative models are accelerating the discovery of new drugs and materials by predicting molecular properties and designing novel compounds.\\\\n    * **Synthetic data generation:** This is becoming critical for training AI models in data-scarce domains, ensuring privacy, and augmenting existing datasets.\\\\n\\\\n5. **Reinforcement Learning (RL) breakthroughs in robotics:** RL is enabling significant progress in robotics, allowing robots to learn complex tasks through trial and error and adapt to dynamic environments.  This includes improvements in robot manipulation, locomotion, and navigation, pushing the boundaries of autonomous systems.  Simulation-to-reality transfer is a key area of focus to accelerate learning and reduce the reliance on real-world data.\\\\n\\\\n6. **Focus on AI Safety and Ethics:**  The growing capabilities of AI systems have highlighted the importance of ethical considerations and safety protocols.  Research is focused on developing techniques to mitigate biases, ensure fairness, and prevent malicious use.  This includes developing robust methods for detecting and preventing adversarial attacks and creating frameworks for responsible AI development and deployment.\\\\n\\\\n7. **Edge AI advancements:**  The increasing power and efficiency of edge devices (smartphones, IoT devices) are enabling the deployment of more sophisticated AI models directly on these devices. This reduces latency, improves privacy, and enables real-time applications in contexts with limited connectivity.\\\\n\\\\n8. **Neuro-Symbolic AI:** This emerging field aims to combine the strengths of neural networks (learning from data) and symbolic AI (reasoning and knowledge representation). This approach promises to create more robust, explainable, and knowledge-rich AI systems, capable of handling complex reasoning tasks.\\\\n\\\\n9. **Advances in Computer Vision for 3D understanding:**  Computer vision is advancing beyond 2D image processing, with a greater focus on 3D scene understanding.  This includes improvements in depth estimation, 3D object detection, and scene reconstruction, facilitating applications in autonomous driving, robotics, and augmented reality.\\\\n\\\\n10. **Increased focus on data efficiency:**  Research is increasingly focusing on developing AI models that require less data for training. This addresses the challenges of data scarcity, cost, and privacy, making AI more accessible and applicable to a wider range of problems.  Techniques like transfer learning, few-shot learning, and meta-learning are playing a key role.\\\", \\\"coworker\\\": \\\"Writer\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "The past year has witnessed significant advancements across various AI domains.  Large Language Models (LLMs) show improved reasoning, multilingual capabilities, and reduced biases, driven by techniques like RLHF and instruction tuning.  Multimodal AI integrates text, images, and audio for more natural interactions, while Explainable AI (XAI) focuses on enhancing transparency and interpretability. Generative AI excels in diverse applications, including generative design, drug discovery, and synthetic data generation.  Reinforcement Learning (RL) breakthroughs are propelling progress in robotics, particularly in manipulation and locomotion.  A strong emphasis on AI safety and ethics guides research to mitigate biases and prevent malicious use. Edge AI empowers the deployment of sophisticated models on resource-constrained devices.  Neuro-symbolic AI aims to combine the strengths of neural networks and symbolic AI for more robust systems.  Computer vision is progressing toward 3D scene understanding, and a growing focus on data efficiency is making AI more accessible through techniques like transfer learning.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mProject Manager\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "The past year has witnessed significant advancements across various AI domains.  Large Language Models (LLMs) show improved reasoning, multilingual capabilities, and reduced biases, driven by techniques like RLHF and instruction tuning.  Multimodal AI integrates text, images, and audio for more natural interactions, while Explainable AI (XAI) focuses on enhancing transparency and interpretability. Generative AI excels in diverse applications, including generative design, drug discovery, and synthetic data generation.  Reinforcement Learning (RL) breakthroughs are propelling progress in robotics, particularly in manipulation and locomotion.  A strong emphasis on AI safety and ethics guides research to mitigate biases and prevent malicious use. Edge AI empowers the deployment of sophisticated models on resource-constrained devices.  Neuro-symbolic AI aims to combine the strengths of neural networks and symbolic AI for more robust systems.  Computer vision is progressing toward 3D scene understanding, and a growing focus on data efficiency is making AI more accessible through techniques like transfer learning.\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "The past year has witnessed significant advancements across various AI domains.  Large Language Models (LLMs) show improved reasoning, multilingual capabilities, and reduced biases, driven by techniques like RLHF and instruction tuning.  Multimodal AI integrates text, images, and audio for more natural interactions, while Explainable AI (XAI) focuses on enhancing transparency and interpretability. Generative AI excels in diverse applications, including generative design, drug discovery, and synthetic data generation.  Reinforcement Learning (RL) breakthroughs are propelling progress in robotics, particularly in manipulation and locomotion.  A strong emphasis on AI safety and ethics guides research to mitigate biases and prevent malicious use. Edge AI empowers the deployment of sophisticated models on resource-constrained devices.  Neuro-symbolic AI aims to combine the strengths of neural networks and symbolic AI for more robust systems.  Computer vision is progressing toward 3D scene understanding, and a growing focus on data efficiency is making AI more accessible through techniques like transfer learning.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "\n",
    "# Set the API key\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyABZOPrSLEGtDNroZPk_94PfHV4f-5-luY\"\n",
    "\n",
    "# Define the manager agent\n",
    "manager = Agent(\n",
    "    role='Project Manager',\n",
    "    goal='Efficiently manage the crew and ensure task completion',\n",
    "    backstory='You are an experienced project manager skilled in coordinating teams and achieving goals.',\n",
    "    llm='gemini/gemini-1.5-flash',\n",
    "    allow_delegation=True\n",
    ")\n",
    "\n",
    "# Define other agents\n",
    "researcher = Agent(\n",
    "    role='Researcher',\n",
    "    goal='Find the latest developments in AI',\n",
    "    backstory='You are an experienced researcher with a knack for uncovering the latest trends in AI.',\n",
    "    llm='gemini/gemini-1.5-flash'\n",
    ")\n",
    "\n",
    "writer = Agent(\n",
    "    role='Writer',\n",
    "    goal='Write a concise summary of the research findings',\n",
    "    backstory='You are a skilled writer known for creating engaging and clear content.',\n",
    "    llm='gemini/gemini-1.5-flash'\n",
    ")\n",
    "\n",
    "# Define tasks\n",
    "research_task = Task(\n",
    "    description='Research the latest developments in AI',\n",
    "    expected_output='A list of key findings',\n",
    "    agent=researcher\n",
    ")\n",
    "\n",
    "write_task = Task(\n",
    "    description='Write a summary based on the research findings',\n",
    "    expected_output='A concise summary',\n",
    "    agent=writer\n",
    ")\n",
    "\n",
    "# Create the crew with hierarchical process and custom manager\n",
    "crew = Crew(\n",
    "    agents=[researcher, writer],\n",
    "    tasks=[research_task, write_task],\n",
    "    process=Process.hierarchical,\n",
    "    manager_agent=manager,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the crew\n",
    "result = crew.kickoff()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "782872c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mManager\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mGenerate a UniProt query from the protein function description.\u001b[00m\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92muniprot_query_generator\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mWhat is the protein function description that should be used to generate the UniProt query?\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92muniprot_query_generator\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mUniProt Query Tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": {\\\"description\\\": \\\"protein kinase activity\\\", \\\"type\\\": \\\"Any\\\"}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'results': ['protein1', 'protein2'], 'query': {'description': 'protein kinase activity', 'type': 'Any'}}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92muniprot_query_generator\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "protein kinase activity\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mManager\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mAsk question to coworker\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"question\\\": \\\"What is the protein function description that should be used to generate the UniProt query?\\\", \\\"context\\\": \\\"We need to generate a UniProt query based on a protein function description.  Please provide the description.\\\", \\\"coworker\\\": \\\"uniprot_query_generator\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "protein kinase activity\n",
      "```\u001b[00m\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92muniprot_query_generator\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mGenerate a UniProt query string based on the provided protein function description.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92muniprot_query_generator\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mUniProt Query Tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": {\\\"description\\\": \\\"protein kinase activity\\\"}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'results': ['protein1', 'protein2'], 'query': {'description': 'protein kinase activity'}}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92muniprot_query_generator\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mUniProt Query Tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": {\\\"description\\\": \\\"protein kinase OR kinase activity OR ATP binding OR phosphorylation\\\"}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'results': ['protein1', 'protein2'], 'query': {'description': 'protein kinase OR kinase activity OR ATP binding OR phosphorylation'}}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92muniprot_query_generator\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "\"protein kinase OR kinase activity OR ATP binding OR phosphorylation\"\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mManager\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mDelegate work to coworker\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"task\\\": \\\"Generate a UniProt query string based on the provided protein function description.\\\", \\\"context\\\": \\\"The protein function description is: 'protein kinase activity'.  Use this description to construct a UniProt query string that will effectively retrieve relevant protein entries.  Focus on keywords and synonyms that accurately reflect the protein kinase activity.\\\", \\\"coworker\\\": \\\"uniprot_query_generator\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "\"protein kinase OR kinase activity OR ATP binding OR phosphorylation\"\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mManager\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "\"protein kinase OR kinase activity OR ATP binding OR phosphorylation\"\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mManager\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mReview and validate the UniProt query.\u001b[00m\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mquery_assurance_agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mReview and validate the UniProt query for accuracy and completeness.  The query is intended to identify proteins related to kinase activity.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mquery_assurance_agent\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92m```\n",
      "Thought:The provided query string is a good starting point but might be too broad and could miss some relevant proteins or include irrelevant ones.  \"Protein kinase\" is specific, but the others might be too broad and pull in unrelated proteins.  I should test the query and see what comes up, then refine it.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mUniProt Query Tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": {\\\"description\\\": \\\"protein kinase OR kinase activity OR ATP binding OR phosphorylation\\\", \\\"type\\\": \\\"Any\\\"}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'results': ['protein1', 'protein2'], 'query': {'description': 'protein kinase OR kinase activity OR ATP binding OR phosphorylation', 'type': 'Any'}}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mquery_assurance_agent\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought:The observation is just a placeholder. The actual results would be a large list.  To assess this properly, I need the actual results to see if any false positives or negatives exist.  However, based on my knowledge, \"kinase activity\" and \"ATP binding\" are too broad. Many proteins bind ATP without kinase activity, and \"kinase activity\" is a function, not a readily searchable annotation in UniProt.  We should focus on keywords directly related to the UniProt database's annotation system. We can improve this by using more precise keywords and possibly adding some controlled vocabularies from UniProt's keyword lists. I'll refine the query using more specific terms and focus on UniProt's controlled vocabulary terms, using more specific annotation terms instead of general functions like \"kinase activity\".\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mUniProt Query Tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"query\\\": {\\\"description\\\": \\\"kinase AND ((catalytic activity \\\\\\\"protein kinase\\\\\\\") OR (keyword:\\\\\\\"protein kinase\\\\\\\"))\\\", \\\"type\\\": \\\"Any\\\"}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'results': ['protein1', 'protein2'], 'query': {'description': 'kinase AND ((catalytic activity \"protein kinase\") OR (keyword:\"protein kinase\"))', 'type': 'Any'}}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mquery_assurance_agent\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "The initial query \"protein kinase OR kinase activity OR ATP binding OR phosphorylation\" is too broad and might return many irrelevant results.  The term \"kinase activity\" is particularly problematic as it's a functional description not easily searchable within UniProt's structured data.  A significantly improved query is:  \"kinase AND ((catalytic activity \\\"protein kinase\\\") OR (keyword:\\\"protein kinase\\\"))\". This revised query uses more precise terms and UniProt's controlled vocabularies to improve specificity and relevance.  While this refined query offers substantial improvements, further iterative refinement based on actual query results might still be needed to perfect the retrieval of all relevant proteins.\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mManager\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mDelegate work to coworker\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"task\\\": \\\"Review and validate the UniProt query for accuracy and completeness.  The query is intended to identify proteins related to kinase activity.\\\", \\\"context\\\": \\\"The UniProt query string is: \\\\\\\"protein kinase OR kinase activity OR ATP binding OR phosphorylation\\\\\\\". Please assess if this query is likely to retrieve all relevant results and if there are any potential issues with its specificity or comprehensiveness.  Consider whether any terms are too broad or too narrow, and suggest improvements if necessary. Your response should be a report approving the query or suggesting improvements.\\\", \\\"coworker\\\": \\\"query_assurance_agent\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "The initial query \"protein kinase OR kinase activity OR ATP binding OR phosphorylation\" is too broad and might return many irrelevant results.  The term \"kinase activity\" is particularly problematic as it's a functional description not easily searchable within UniProt's structured data.  A significantly improved query is:  \"kinase AND ((catalytic activity \\\"protein kinase\\\") OR (keyword:\\\"protein kinase\\\"))\". This revised query uses more precise terms and UniProt's controlled vocabularies to improve specificity and relevance.  While this refined query offers substantial improvements, further iterative refinement based on actual query results might still be needed to perfect the retrieval of all relevant proteins.\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mManager\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mNone\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "I encountered an error: Action 'None' don't exist, these are the only available Actions:\n",
      "Tool Name: UniProt Query Tool\n",
      "Tool Arguments: {'query': {'description': '', 'type': 'Any'}}\n",
      "Tool Description: Queries the UniProt database to retrieve protein information.\n",
      "Tool Name: Delegate work to coworker\n",
      "Tool Arguments: {'task': {'description': 'The task to delegate', 'type': 'str'}, 'context': {'description': 'The context for the task', 'type': 'str'}, 'coworker': {'description': 'The role/name of the coworker to delegate to', 'type': 'str'}}\n",
      "Tool Description: Delegate a specific task to one of the following coworkers: query_assurance_agent\n",
      "The input to this tool should be the coworker, the task you want them to do, and ALL necessary context to execute the task, they know nothing about the task, so share absolutely everything you know, don't reference things but instead explain them.\n",
      "Tool Name: Ask question to coworker\n",
      "Tool Arguments: {'question': {'description': 'The question to ask', 'type': 'str'}, 'context': {'description': 'The context for the question', 'type': 'str'}, 'coworker': {'description': 'The role/name of the coworker to ask', 'type': 'str'}}\n",
      "Tool Description: Ask a specific question to one of the following coworkers: query_assurance_agent\n",
      "The input to this tool should be the coworker, the question you have for them, and ALL necessary context to ask the question properly, they know nothing about the question, so share absolutely everything you know, don't reference things but instead explain them.\n",
      "Moving on then. I MUST either use a tool (use one at time) OR give my best final answer not both at the same time. When responding, I must use the following format:\n",
      "\n",
      "```\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [UniProt Query Tool, Delegate work to coworker, Ask question to coworker]\n",
      "Action Input: the input to the action, dictionary enclosed in curly braces\n",
      "Observation: the result of the action\n",
      "```\n",
      "This Thought/Action/Action Input/Result can repeat N times. Once I know the final answer, I must return the following format:\n",
      "\n",
      "```\n",
      "Thought: I now can give a great answer\n",
      "Final Answer: Your final answer must be the great and the most complete as possible, it must be outcome described\n",
      "\n",
      "```\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mManager\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "## UniProt Query Review Report\n",
      "\n",
      "**Original Query:** \"protein kinase OR kinase activity OR ATP binding OR phosphorylation\"\n",
      "\n",
      "**Assessment:** The original query, while seemingly comprehensive, suffers from several drawbacks that could lead to a high number of false positives and a low recall of relevant proteins.  The term \"kinase activity\" is a functional description, and UniProt's search functionality is more effective when leveraging structured data like gene ontology terms or keywords.  Using \"OR\" broadly connects terms with varying specificity, potentially resulting in an excessively broad search.\n",
      "\n",
      "**Proposed Improvements:** The query_assurance_agent suggests the following revised query:  \"kinase AND ((catalytic activity \\\"protein kinase\\\") OR (keyword:\\\"protein kinase\\\"))\". This modified query utilizes more specific terminology and leverages UniProt's structured fields (\"catalytic activity\" and \"keyword\") to improve search precision and retrieve more accurate and relevant results.  The use of \"AND\" ensures that only proteins annotated as \"kinase\" and possessing either the specified catalytic activity or keyword are included, mitigating the risk of irrelevant hits.\n",
      "\n",
      "\n",
      "**Recommendation:** While the revised query provides a substantial improvement, it's recommended to test the query with small sets of known relevant and irrelevant proteins to fine-tune its precision and recall.  Further refinements might be needed based on the evaluation of retrieved results.  The revised query offers a significantly better starting point compared to the initial query, however, and represents a robust approach to retrieving proteins related to kinase activity.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mManager\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mAnalyze protein features and determine motifs to preserve or mask.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:LiteLLM call failed: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"location\": \"global\",\n",
      "              \"model\": \"gemini-1.5-flash\"\n",
      "            },\n",
      "            \"quotaValue\": \"15\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"32s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\u001b[91m Error during LLM call: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n",
      "        \"violations\": [\n",
      "          {\n",
      "            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n",
      "            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n",
      "            \"quotaDimensions\": {\n",
      "              \"location\": \"global\",\n",
      "              \"model\": \"gemini-1.5-flash\"\n",
      "            },\n",
      "            \"quotaValue\": \"15\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Learn more about Gemini API quotas\",\n",
      "            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n",
      "        \"retryDelay\": \"32s\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[00m\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-1.5-flash\"\n            },\n            \"quotaValue\": \"15\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"32s\"\n      }\n    ]\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py:1282\u001b[0m, in \u001b[0;36mVertexLLM.completion\u001b[1;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1282\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:553\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:534\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[0;32m    533\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m--> 534\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\httpx\\_models.py:763\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 763\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyABZOPrSLEGtDNroZPk_94PfHV4f-5-luY'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mVertexAIError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\main.py:2299\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   2298\u001b[0m     new_params \u001b[38;5;241m=\u001b[39m deepcopy(optional_params)\n\u001b[1;32m-> 2299\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mvertex_chat_completion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   2300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2304\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   2306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_ai_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2313\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py:1286\u001b[0m, in \u001b[0;36mVertexLLM.completion\u001b[1;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[0;32m   1285\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[1;32m-> 1286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[0;32m   1287\u001b[0m         status_code\u001b[38;5;241m=\u001b[39merror_code,\n\u001b[0;32m   1288\u001b[0m         message\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   1289\u001b[0m         headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1290\u001b[0m     )\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException:\n",
      "\u001b[1;31mVertexAIError\u001b[0m: {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-1.5-flash\"\n            },\n            \"quotaValue\": \"15\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"32s\"\n      }\n    ]\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 116\u001b[0m\n\u001b[0;32m    107\u001b[0m crew \u001b[38;5;241m=\u001b[39m Crew(\n\u001b[0;32m    108\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[query_generator, uniprot_query_assurance_agent, protein_expert_agent, rf_diffusion_expert],\n\u001b[0;32m    109\u001b[0m     tasks\u001b[38;5;241m=\u001b[39m[plan_task, query_review_task, protein_analysis_task, rf_diffusion_task],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Fixed from verbose=2 to verbose=True\u001b[39;00m\n\u001b[0;32m    113\u001b[0m )\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Run the crew with an example input\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcrew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkickoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprotein_function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcatalyzes the hydrolysis of peptide bonds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\crew.py:578\u001b[0m, in \u001b[0;36mkickoff\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    569\u001b[0m             result \u001b[38;5;241m=\u001b[39m TaskEvaluator(agent)\u001b[38;5;241m.\u001b[39mevaluate_training_data(\n\u001b[0;32m    570\u001b[0m                 training_data\u001b[38;5;241m=\u001b[39mtraining_data, agent_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(agent\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m    571\u001b[0m             )\n\u001b[0;32m    572\u001b[0m             CrewTrainingHandler(filename)\u001b[38;5;241m.\u001b[39msave_trained_data(\n\u001b[0;32m    573\u001b[0m                 agent_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(agent\u001b[38;5;241m.\u001b[39mrole), trained_data\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[0;32m    574\u001b[0m             )\n\u001b[0;32m    576\u001b[0m     crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m--> 578\u001b[0m         CrewTrainCompletedEvent(\n\u001b[0;32m    579\u001b[0m             crew_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrew\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    580\u001b[0m             n_iterations\u001b[38;5;241m=\u001b[39mn_iterations,\n\u001b[0;32m    581\u001b[0m             filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    582\u001b[0m         ),\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    585\u001b[0m     crewai_event_bus\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m    586\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    587\u001b[0m         CrewTrainFailedEvent(error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e), crew_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrew\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    588\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\crew.py:688\u001b[0m, in \u001b[0;36m_run_hierarchical_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\crew.py:781\u001b[0m, in \u001b[0;36mCrew._execute_tasks\u001b[1;34m(self, tasks, start_index, was_replayed)\u001b[0m\n\u001b[0;32m    778\u001b[0m     futures\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    780\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_context(task, task_outputs)\n\u001b[1;32m--> 781\u001b[0m task_output \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_for_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m task_outputs\u001b[38;5;241m.\u001b[39mappend(task_output)\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_task_result(task, task_output)\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\task.py:302\u001b[0m, in \u001b[0;36mTask.execute_sync\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_sync\u001b[39m(\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    297\u001b[0m     agent: Optional[BaseAgent] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    298\u001b[0m     context: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m     tools: Optional[List[BaseTool]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    300\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskOutput:\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the task synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\task.py:366\u001b[0m, in \u001b[0;36mTask._execute_core\u001b[1;34m(self, agent, context, tools)\u001b[0m\n\u001b[0;32m    362\u001b[0m tools \u001b[38;5;241m=\u001b[39m tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_by_agents\u001b[38;5;241m.\u001b[39madd(agent\u001b[38;5;241m.\u001b[39mrole)\n\u001b[1;32m--> 366\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m pydantic_output, json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_output(result)\n\u001b[0;32m    373\u001b[0m task_output \u001b[38;5;241m=\u001b[39m TaskOutput(\n\u001b[0;32m    374\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    375\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdescription,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     output_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_format(),\n\u001b[0;32m    382\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\agent.py:254\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_times_executed \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retry_limit:\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\agent.py:243\u001b[0m, in \u001b[0;36mAgent.execute_task\u001b[1;34m(self, task, context, tools)\u001b[0m\n\u001b[0;32m    240\u001b[0m     task_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_trained_data(task_prompt\u001b[38;5;241m=\u001b[39mtask_prompt)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask_for_human_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuman_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:112\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_unknown_error(e)\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:102\u001b[0m, in \u001b[0;36mCrewAgentExecutor.invoke\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mask_for_human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mask_for_human_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    105\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent failed to reach a final answer. This is likely a bug - please report it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:160\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Do not retry on litellm errors\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_context_length_exceeded(e):\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_context_length()\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:140\u001b[0m, in \u001b[0;36mCrewAgentExecutor._invoke_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_rpm_limit()\n\u001b[1;32m--> 140\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m formatted_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_llm_response(answer)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatted_answer, AgentAction):\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:210\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    207\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    208\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m answer:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    214\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived None or empty response from LLM call.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\agents\\crew_agent_executor.py:201\u001b[0m, in \u001b[0;36mCrewAgentExecutor._get_llm_response\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the LLM and return the response, handling any invalid responses.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer\u001b[38;5;241m.\u001b[39mprint(\n\u001b[0;32m    207\u001b[0m         content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during LLM call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    208\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\crewai\\llm.py:291\u001b[0m, in \u001b[0;36mLLM.call\u001b[1;34m(self, messages, tools, callbacks, available_functions)\u001b[0m\n\u001b[0;32m    288\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# --- 2) Make the completion call\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m response_message \u001b[38;5;241m=\u001b[39m cast(Choices, cast(ModelResponse, response)\u001b[38;5;241m.\u001b[39mchoices)[\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    294\u001b[0m ]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m    295\u001b[0m text_response \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\utils.py:1154\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1151\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1152\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1153\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1154\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\utils.py:1032\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 1032\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1033\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\main.py:3068\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   3066\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3067\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 3068\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3071\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2201\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   2199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[0;32m   2200\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[1;32m-> 2201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   2202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[1;32mc:\\Users\\91900\\radioconda\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:1243\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[0;32m   1242\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[0;32m   1244\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm.RateLimitError: VertexAIException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1245\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1246\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1247\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[0;32m   1248\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[0;32m   1249\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m429\u001b[39m,\n\u001b[0;32m   1250\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m   1251\u001b[0m                 method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1252\u001b[0m                 url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://cloud.google.com/vertex-ai/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1253\u001b[0m             ),\n\u001b[0;32m   1254\u001b[0m         ),\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m   1257\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\",\n    \"status\": \"RESOURCE_EXHAUSTED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n        \"violations\": [\n          {\n            \"quotaMetric\": \"generativelanguage.googleapis.com/generate_content_free_tier_requests\",\n            \"quotaId\": \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\",\n            \"quotaDimensions\": {\n              \"location\": \"global\",\n              \"model\": \"gemini-1.5-flash\"\n            },\n            \"quotaValue\": \"15\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Learn more about Gemini API quotas\",\n            \"url\": \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n          }\n        ]\n      },\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.RetryInfo\",\n        \"retryDelay\": \"32s\"\n      }\n    ]\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from langchain.tools import Tool  # Import Tool class from LangChain\n",
    "\n",
    "# Set the API key for the Gemini model (replace with your actual key)\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyABZOPrSLEGtDNroZPk_94PfHV4f-5-luY\"\n",
    "\n",
    "# Define simple placeholder tool functions\n",
    "def uniprot_tool(query):\n",
    "    \"\"\"Simulate querying the UniProt database.\"\"\"\n",
    "    return {\"results\": [\"protein1\", \"protein2\"], \"query\": query}\n",
    "\n",
    "def get_protein_site_info(protein_id):\n",
    "    \"\"\"Simulate retrieving protein site information.\"\"\"\n",
    "    return {\"sites\": [{\"type\": \"active\", \"position\": 42}, {\"type\": \"binding\", \"position\": 78}]}\n",
    "\n",
    "# Define tools as Tool instances\n",
    "uniprot_tool_instance = Tool(\n",
    "    name=\"UniProt Query Tool\",\n",
    "    func=uniprot_tool,\n",
    "    description=\"Queries the UniProt database to retrieve protein information.\"\n",
    ")\n",
    "\n",
    "get_protein_site_info_instance = Tool(\n",
    "    name=\"Protein Site Info Tool\",\n",
    "    func=get_protein_site_info,\n",
    "    description=\"Retrieves site information for a given protein.\"\n",
    ")\n",
    "\n",
    "# Define agents\n",
    "query_generator = Agent(\n",
    "    role=\"uniprot_query_generator\",\n",
    "    goal=\"Generate a UniProt query from a protein function.\",\n",
    "    backstory=\"A bioinformatics assistant for creating UniProt queries.\",\n",
    "    tools=[uniprot_tool_instance],\n",
    "    verbose=True,\n",
    "    llm=\"gemini/gemini-1.5-flash\"  # Ensure this LLM is supported by your setup\n",
    ")\n",
    "\n",
    "uniprot_query_assurance_agent = Agent(\n",
    "    role=\"query_assurance_agent\",\n",
    "    goal=\"Ensure the UniProt query is accurate and relevant.\",\n",
    "    backstory=\"A quality control specialist for bioinformatics queries.\",\n",
    "    tools=[uniprot_tool_instance],\n",
    "    verbose=True,\n",
    "    llm=\"gemini/gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "protein_expert_agent = Agent(\n",
    "    role=\"Protein Scaffolding Specialist\",\n",
    "    goal=\"Identify motifs to mask or preserve based on UniProt features.\",\n",
    "    backstory=\"Expert in protein design and analysis.\",\n",
    "    tools=[get_protein_site_info_instance],\n",
    "    verbose=True,\n",
    "    llm=\"gemini/gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "rf_diffusion_expert = Agent(\n",
    "    role=\"RF Diffusion Expert\",\n",
    "    goal=\"Translate protein scaffolding requirements into RF Diffusion strategies.\",\n",
    "    backstory=\"Specialist in diffusion-based protein modeling.\",\n",
    "    verbose=True,\n",
    "    llm=\"gemini/gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "manager = Agent(\n",
    "    role=\"Manager\",\n",
    "    goal=\"Coordinate the workflow efficiently.\",\n",
    "    backstory=\"Experienced in managing bioinformatics workflows.\",\n",
    "    llm=\"gemini/gemini-1.5-flash\",\n",
    "    allow_delegation=True\n",
    ")\n",
    "\n",
    "# Define tasks\n",
    "plan_task = Task(\n",
    "    name=\"plan\",\n",
    "    description=\"Generate a UniProt query from the protein function description.\",\n",
    "    expected_output=\"A UniProt query string.\",\n",
    "    agent=query_generator\n",
    ")\n",
    "\n",
    "query_review_task = Task(\n",
    "    name=\"query_review\",\n",
    "    description=\"Review and validate the UniProt query.\",\n",
    "    expected_output=\"A report approving the query or suggesting improvements.\",\n",
    "    agent=uniprot_query_assurance_agent,\n",
    "    context=[plan_task]\n",
    ")\n",
    "\n",
    "protein_analysis_task = Task(\n",
    "    name=\"protein_analysis\",\n",
    "    description=\"Analyze protein features and determine motifs to preserve or mask.\",\n",
    "    expected_output=\"A JSON report with motif analysis and scaffolding approach.\",\n",
    "    agent=protein_expert_agent,\n",
    "    context=[query_review_task]\n",
    ")\n",
    "\n",
    "rf_diffusion_task = Task(\n",
    "    name=\"rf_diffusion_config\",\n",
    "    description=\"Generate an RF Diffusion configuration script.\",\n",
    "    expected_output=\"A terminal script for RF Diffusion.\",\n",
    "    agent=rf_diffusion_expert,\n",
    "    context=[protein_analysis_task]\n",
    ")\n",
    "\n",
    "# Create the crew with a hierarchical process\n",
    "crew = Crew(\n",
    "    agents=[query_generator, uniprot_query_assurance_agent, protein_expert_agent, rf_diffusion_expert],\n",
    "    tasks=[plan_task, query_review_task, protein_analysis_task, rf_diffusion_task],\n",
    "    process=Process.hierarchical,\n",
    "    manager_agent=manager,\n",
    "    verbose=True  # Fixed from verbose=2 to verbose=True\n",
    ")\n",
    "\n",
    "# Run the crew with an example input\n",
    "result = crew.kickoff(inputs={\"protein_function\": \"catalyzes the hydrolysis of peptide bonds\"})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
